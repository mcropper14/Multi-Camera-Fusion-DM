
# Adaptive Multi-Camera Sensor Fusion for Autonomous Vehicles

This project implements a multi-modal sensor fusion pipeline to predict the future position and orientation (GNSS pose) of an autonomous vehicle using:

- Vision Transformers (ViT) for multi-camera feature extraction
- Multi-Head Latent Attention (MLA) for cross-view fusion
- GNSS (latitude, longitude, altitude, roll, pitch, azimuth) for spatial grounding
- A BiLSTM-based regression model for future GNSS trajectory prediction

ðŸ“ Tested on the Canadian Adverse Driving Conditions (CADC) dataset.

---

## ðŸ”§ Project Structure

```bash
â”œâ”€â”€ camera_info/                 # Calibration utilities
â”œâ”€â”€ cnn_baseline/               # CNN-based baseline for GNSS prediction
â”œâ”€â”€ coordinate_transformation/  # GPS -> XYZ coordinate transforms
â”œâ”€â”€ data_loading/               # Matched data loaders
â”œâ”€â”€ vit_gnss_nomla/             # ViT + GNSS (no fusion)
â”œâ”€â”€ vit_mla_gnss/               # ViT + MLA + GNSS + LSTM (main pipeline)
â”‚   â”œâ”€â”€ eval.py                 # Evaluates trained LSTM model
â”‚   â”œâ”€â”€ eval_no_lstm.py         # Variant for direct regression
â”‚   â”œâ”€â”€ train.py                # Trains LSTM using fused ViT + GNSS(t) -> GNSS(t+1)
â”‚   â”œâ”€â”€ future_gnss_lstm.py     # Full LSTM pipeline
â”‚   â”œâ”€â”€ future_gnss.py          # Feature extractor to generate GNSS prediction data
â”‚   â”œâ”€â”€ vit_fusion_show.py      # Visualizations: attention, t-SNE, saliency maps
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt

##  Method Overview

###  Vision Transformer (ViT)
- Extracts spatial features from two ego cameras (Camera 5 & 6)
- Fine-tuned on CADC for improved winter-road generalization

###  Multi-Head Latent Attention (MLA)
- Aligns and fuses features across camera views
- Learns view-aware attention using latent space alignment

### GNSS Integration
- GNSS at time *t* is concatenated to fused visual features
- Predicts GNSS at time *t+1* (6-DoF pose):  
  `latitude, longitude, altitude, roll, pitch, azimuth`

###  BiLSTM Model
- Captures temporal continuity in vehicle motion
- Predicts future GNSS using a fused feature sequence

---

##  Setup

1. **Install requirements**:
   ```bash
   pip install -r requirements.txt

2. Download CADC Dataset [View the CADC dataset](https://github.com/mpitropov/cadc_devkit/tree/master)


3. Ensure the Directory is setup as:
/path/to/cadc/
  â””â”€â”€ sequence_01/
      â”œâ”€â”€ image_05/
      â”œâ”€â”€ image_06/
      â””â”€â”€ gnss.json

4. Run load_data.py for data data synchronization 

## Pipeline 

1. Extract Features + gnss: python vit_mla_gnss/future_gnss.py

2. Train for GNSS Prediction: python vit_mla_gnss/future_gnss.py

3. Evaluate Model Accuracy: python vit_mla_gnss/eval.py

